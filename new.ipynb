{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf         \n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid OOM errors by setting GPU Memory Consumption Growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"new_final.csv\",encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ex Wife Threatening SuicideRecently I left my ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Am I weird I don't get affected by compliments...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Finally 2020 is almost over... So I can never ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i need helpjust help me im crying so hard</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IÃ¢â¬â¢m so lostHello, my name is Adam (16) ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233965</th>\n",
       "      <td>\"Living has become unbearable. The pain is too...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233966</th>\n",
       "      <td>\"The darkness and despair have consumed every ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233967</th>\n",
       "      <td>\"I can't see any way out of this overwhelming ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233968</th>\n",
       "      <td>\"The relentless cycle of despair makes every d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233969</th>\n",
       "      <td>\"I feel completely alone and worthless. The pa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>233970 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text class\n",
       "0       Ex Wife Threatening SuicideRecently I left my ...     1\n",
       "1       Am I weird I don't get affected by compliments...     0\n",
       "2       Finally 2020 is almost over... So I can never ...     0\n",
       "3               i need helpjust help me im crying so hard     1\n",
       "4       IÃ¢â¬â¢m so lostHello, my name is Adam (16) ...     1\n",
       "...                                                   ...   ...\n",
       "233965  \"Living has become unbearable. The pain is too...     1\n",
       "233966  \"The darkness and despair have consumed every ...     1\n",
       "233967  \"I can't see any way out of this overwhelming ...     1\n",
       "233968  \"The relentless cycle of despair makes every d...     1\n",
       "233969  \"I feel completely alone and worthless. The pa...     1\n",
       "\n",
       "[233970 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ex Wife Threatening SuicideRecently I left my ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Am I weird I don't get affected by compliments...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Finally 2020 is almost over... So I can never ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i need helpjust help me im crying so hard</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IÃ¢â¬â¢m so lostHello, my name is Adam (16) ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233965</th>\n",
       "      <td>\"Living has become unbearable. The pain is too...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233966</th>\n",
       "      <td>\"The darkness and despair have consumed every ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233967</th>\n",
       "      <td>\"I can't see any way out of this overwhelming ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233968</th>\n",
       "      <td>\"The relentless cycle of despair makes every d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233969</th>\n",
       "      <td>\"I feel completely alone and worthless. The pa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>233946 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text class\n",
       "0       Ex Wife Threatening SuicideRecently I left my ...     1\n",
       "1       Am I weird I don't get affected by compliments...     0\n",
       "2       Finally 2020 is almost over... So I can never ...     0\n",
       "3               i need helpjust help me im crying so hard     1\n",
       "4       IÃ¢â¬â¢m so lostHello, my name is Adam (16) ...     1\n",
       "...                                                   ...   ...\n",
       "233965  \"Living has become unbearable. The pain is too...     1\n",
       "233966  \"The darkness and despair have consumed every ...     1\n",
       "233967  \"I can't see any way out of this overwhelming ...     1\n",
       "233968  \"The relentless cycle of despair makes every d...     1\n",
       "233969  \"I feel completely alone and worthless. The pa...     1\n",
       "\n",
       "[233946 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~data[\"class\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "class    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data[\"class\"]!=\" Pepe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "1    117194\n",
       "0    116751\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data[\"text\"].to_list()\n",
    "sentiment = data[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, vocab_path, max_length=100) -> None:\n",
    "        with open(vocab_path, \"r\") as stream:\n",
    "            self.vocab = json.load(stream)\n",
    "        self.len = self.vocab[\"vocab_size\"]\n",
    "        self.word_to_index = self.vocab[\"word_to_index\"]\n",
    "        self.index_to_word = self.vocab[\"index_to_word\"]\n",
    "        self.pad_id = self.word_to_index[\"<PAD>\"]\n",
    "        self.start_id = self.word_to_index[\"<S>\"]\n",
    "        self.end_id = self.word_to_index[\"</S>\"]\n",
    "        self.max_length = max_length\n",
    "        self.negate = self.word_to_index['negate']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def combos(self, s, first=False):\n",
    "        if not s:\n",
    "            return\n",
    "        length = len(s)\n",
    "        if not first:\n",
    "            yield [s]\n",
    "        for i in range(1, length):\n",
    "            for c in self.combos(s[: length - i]):\n",
    "                yield c + [s[length - i:]]\n",
    "\n",
    "    def get_indx(self, combos: list):\n",
    "        indx_of_combo = -1\n",
    "        for i, combo in enumerate(combos):\n",
    "            cond = True\n",
    "            for ele in combo:\n",
    "                if ele not in self.word_to_index:\n",
    "                    cond = False\n",
    "                    break\n",
    "            if cond:\n",
    "                indx_of_combo = i\n",
    "                break\n",
    "        indx = []\n",
    "        if indx_of_combo != -1:\n",
    "            for ele in combos[indx_of_combo]:\n",
    "                indx.append(self.word_to_index[ele])\n",
    "        return indx\n",
    "\n",
    "    def split_word(self, word: str):\n",
    "        combos = []\n",
    "        for c in self.combos(word, True):\n",
    "            size = len(c)\n",
    "            c[size - 1] = c[size - 1] + \"</w>\"\n",
    "            combos.append(c)\n",
    "        combos = sorted(combos, key=lambda x: len(x))\n",
    "        indx = self.get_indx(combos)\n",
    "        return indx\n",
    "\n",
    "    def create_pad(self, x: list):\n",
    "        max = self.max_length\n",
    "        if isinstance(x[0], int):\n",
    "            while len(x) < max:\n",
    "                x.append(self.word_to_index[\"<PAD>\"])\n",
    "        else:\n",
    "            for i, line in enumerate(x):\n",
    "                if len(line) > max:\n",
    "                    x[i] = x[i][:max]\n",
    "                while len(line) < max:\n",
    "                    x[i].append(self.word_to_index[\"<PAD>\"])\n",
    "        return x\n",
    "\n",
    "    def handle_negations(self, text):\n",
    "        # Expand contractions first\n",
    "        #text = contractions.fix(text)\n",
    "        try :\n",
    "            # Handle negations\n",
    "            pattern = re.compile(r\"\\b(?:not|no|never|don't|doesn't|can't|won't|shouldn't|couldn't|wouldn't|didn't|isn't|aren't|wasn't|weren't|donot|doesnot|dont|doesnt|cannot|wont|shouldnt|couldnt|wouldnt|didnt|isnt|arent|wasnt|werent)\\b[\\w\\s]*\")\n",
    "\n",
    "            def replace_negation(match):\n",
    "                words = match.group(0).split()\n",
    "                words = [words[0], \"negate\"] + words[1:]\n",
    "                return ' '.join(words)\n",
    "        \n",
    "            processed_text = pattern.sub(replace_negation, text)        \n",
    "            return processed_text\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    def encode(self, x: list):\n",
    "        encoded_x = []\n",
    "        if isinstance(x, list):\n",
    "            for i in tqdm(range(len(x))):\n",
    "                line = self.handle_negations(x[i])\n",
    "                encoded_line = [self.word_to_index[\"<S>\"]]\n",
    "                for word in filter(lambda x: x not in [None, \"\"], re.split(\"\\s+|(\\W{1})\", line)):\n",
    "                    if len(word) > 10:\n",
    "                        continue\n",
    "                    if len(re.findall(\"\\W\", word)) == 0:\n",
    "                        word = f\"{word}</w>\"\n",
    "                    if word in self.word_to_index:\n",
    "                        encoded_line.append(self.word_to_index[word])\n",
    "                    else:\n",
    "                        y = self.split_word(word.replace(\"</w>\", \"\"))\n",
    "                        for indx in y:\n",
    "                            encoded_line.append(indx)\n",
    "                encoded_line.append(self.word_to_index[\"</S>\"])\n",
    "                encoded_x.append(encoded_line)\n",
    "            encoded_x = self.create_pad(encoded_x)\n",
    "\n",
    "        else:\n",
    "            x = self.handle_negations(x)\n",
    "            word_count = 0\n",
    "            encoded_x.append(self.word_to_index[\"<S>\"])\n",
    "            for word in filter(lambda x: x not in [None, \"\"], re.split(\"\\s+|(\\W{1})\", x)):\n",
    "                if len(word) > 10:\n",
    "                    continue\n",
    "                if len(re.findall(\"\\W\", word)) == 0:\n",
    "                    word = f\"{word}</w>\"\n",
    "                if word in self.word_to_index:\n",
    "                    encoded_x.append(self.word_to_index[word])\n",
    "                else:\n",
    "                    y = self.split_word(word.replace(\"</w>\", \"\"))\n",
    "                    for indx in y:\n",
    "                        encoded_x.append(indx)\n",
    "            encoded_x.append(self.word_to_index[\"</S>\"])\n",
    "            encoded_x = self.create_pad(encoded_x)\n",
    "\n",
    "        return np.array(encoded_x)\n",
    "\n",
    "    def decode_step(self, x):\n",
    "        y = []\n",
    "        for indx in x:\n",
    "            word = self.index_to_word[str(int(indx))]\n",
    "            if word.startswith(\"NOT_\"):\n",
    "                word = word.replace(\"NOT_\", \"\")\n",
    "            y.append(word)\n",
    "            \n",
    "        pop_indx = []\n",
    "        for i, word in enumerate(y):\n",
    "            if word in [\"<S>\", \"</S>\", \"<PAD>\", \"</w>\"]:\n",
    "                pop_indx.append(i)\n",
    "                continue\n",
    "            elif len(re.findall(\"\\W\", word.replace(\"</w>\", \"\"))):\n",
    "                y[i - 1] = y[i - 1] + y[i]\n",
    "                pop_indx.append(i)\n",
    "                continue\n",
    "            elif not word.endswith(\"</w>\") and i < len(y) - 1:\n",
    "                y[i + 1] = y[i] + y[i + 1]\n",
    "                pop_indx.append(i)\n",
    "                continue\n",
    "            y[i] = word.replace(\"</w>\", \"\")\n",
    "        for i in reversed(pop_indx):\n",
    "            y.pop(i)\n",
    "        return \" \".join(y)\n",
    "\n",
    "    def decode(self, x):\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = x.tolist()\n",
    "\n",
    "        if isinstance(x[0], list):\n",
    "            y = []\n",
    "            for line in x:\n",
    "                y.append(self.decode_step(line))\n",
    "            return y\n",
    "        else:\n",
    "            return self.decode_step(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['text'] = data['text'].astype(str)\n",
    "# text = data['text'].to_list()\n",
    "tokenizer = Tokenizer('vocab.json')\n",
    "\n",
    "# Preprocess text and tokenize\n",
    "#embeded = tokenizer.encode(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('embeded.npy',embeded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeded = np.load('embeded.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = embeded[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10223"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer('vocab.json')\n",
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_size = len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeding_features = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# X_final=np.array(embeded)\n",
    "# y_final=np.array(data[\"class\"]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_final=np.array(embed)\n",
    "y_final=np.array(data[\"class\"][:1000]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 100), (1000,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_final.shape,y_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ps180\\anaconda3\\envs\\port\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ps180\\anaconda3\\envs\\port\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': X_train}, y_train))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': X_test}, y_test))\n",
    "# Batch and prefetch the datasets\n",
    "train_dataset = train_dataset.shuffle(len(X_train)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/9\n",
      "160/160 [==============================] - 64s 284ms/step - loss: 0.7140 - accuracy: 0.5075 - val_loss: 0.6938 - val_accuracy: 0.4900\n",
      "Epoch 2/9\n",
      "160/160 [==============================] - 44s 278ms/step - loss: 0.6944 - accuracy: 0.5125 - val_loss: 0.6938 - val_accuracy: 0.4900\n",
      "Epoch 3/9\n",
      "160/160 [==============================] - 52s 327ms/step - loss: 0.7037 - accuracy: 0.5150 - val_loss: 0.6930 - val_accuracy: 0.4900\n",
      "Epoch 4/9\n",
      "160/160 [==============================] - 46s 284ms/step - loss: 0.7022 - accuracy: 0.5138 - val_loss: 0.6974 - val_accuracy: 0.4900\n",
      "Epoch 5/9\n",
      "160/160 [==============================] - 44s 278ms/step - loss: 0.6900 - accuracy: 0.5200 - val_loss: 0.6932 - val_accuracy: 0.4900\n",
      "Epoch 6/9\n",
      "160/160 [==============================] - 43s 266ms/step - loss: 0.7007 - accuracy: 0.5113 - val_loss: 0.6932 - val_accuracy: 0.4900\n",
      "Epoch 7/9\n",
      "160/160 [==============================] - 43s 267ms/step - loss: 0.7011 - accuracy: 0.5138 - val_loss: 0.6929 - val_accuracy: 0.4900\n",
      "Epoch 8/9\n",
      "160/160 [==============================] - 43s 267ms/step - loss: 0.6978 - accuracy: 0.5113 - val_loss: 0.6936 - val_accuracy: 0.4900\n",
      "Epoch 9/9\n",
      "160/160 [==============================] - 43s 272ms/step - loss: 0.6965 - accuracy: 0.5150 - val_loss: 0.6948 - val_accuracy: 0.4900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24889d33ee0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(train_dataset, epochs=9, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = predictions.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = tf.nn.sigmoid(logits).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "predicted_labels = (probabilities >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46789, 1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20738,  2613],\n",
       "       [ 1672, 21766]], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test,predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9084186454081088"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test,predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q=\"Having no thoughts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 941.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Having no negate thoughts']\n",
      "[[   0  831 6624 6552 8884 9042    1    2    2    2    2    2    2    2\n",
      "     2    2    2    2    2    2    2    2    2    2    2    2    2    2\n",
      "     2    2    2    2    2    2    2    2    2    2    2    2    2    2\n",
      "     2    2    2    2    2    2    2    2    2    2    2    2    2    2\n",
      "     2    2    2    2    2    2    2    2    2    2    2    2    2    2\n",
      "     2    2    2    2    2    2    2    2    2    2    2    2    2    2\n",
      "     2    2    2    2    2    2    2    2    2    2    2    2    2    2\n",
      "     2    2]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer('vocab.json')\n",
    "\n",
    "single_text = q\n",
    "\n",
    "encoded_text = np.array(tokenizer.encode([single_text]))\n",
    "\n",
    "decoded_text = tokenizer.decode(encoded_text)\n",
    "\n",
    "print(decoded_text)\n",
    "print(encoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = encoded_text\n",
    "attention_mask = np.ones_like(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(inputs, training=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = pred.logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = tf.nn.sigmoid(logits).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "pred_label = (prob >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('Sucidal_classification_t/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at Sucidal_classification_t/ were not used when initializing TFBertForSequenceClassification: ['dropout_37']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at Sucidal_classification_t/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# model = TFBertForSequenceClassification.from_pretrained(\"Sucidal_classification_t/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1810.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Having happy days in my life']\n",
      "[[   0  831 5003 3454 5405 6494 5963    1    2    2    2    2    2    2\n",
      "     2    2    2    2    2    2    2    2    2    2    2    2    2    2\n",
      "     2    2    2    2    2    2    2    2    2    2    2    2    2    2\n",
      "     2    2    2    2    2    2    2    2    2    2    2    2    2    2\n",
      "     2    2    2    2    2    2    2    2    2    2    2    2    2    2\n",
      "     2    2    2    2    2    2    2    2    2    2    2    2    2    2\n",
      "     2    2    2    2    2    2    2    2    2    2    2    2    2    2\n",
      "     2    2]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# q=\"Having happy days in my life\"\n",
    "# tokenizer = Tokenizer('vocab.json')\n",
    "\n",
    "# single_text = q\n",
    "\n",
    "# encoded_text = np.array(tokenizer.encode([single_text]))\n",
    "\n",
    "# decoded_text = tokenizer.decode(encoded_text)\n",
    "\n",
    "# print(decoded_text)\n",
    "# print(encoded_text)\n",
    "\n",
    "# input_ids = encoded_text\n",
    "# attention_mask = np.ones_like(input_ids)\n",
    "# inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "\n",
    "# pred = model(inputs)\n",
    "\n",
    "# logits = pred.logits\n",
    "\n",
    "# prob = tf.nn.sigmoid(logits).numpy()\n",
    "# threshold = 0.5\n",
    "# pred_label = (prob >= threshold).astype(int)\n",
    "# pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "port",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
